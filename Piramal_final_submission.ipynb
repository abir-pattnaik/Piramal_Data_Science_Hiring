{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "['Street Type']\n",
      "Street Type    419\n",
      "dtype: int64\n",
      "['Street Type']\n",
      "Street Type    172\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample code to perform I/O:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "master_train=train\n",
    "master_test=test\n",
    "#print(train_data.head())\n",
    "# Warning: Printing unwanted or ill-formatted data to output will cause the test cases to fail\n",
    "\n",
    "# Write your code here\n",
    "## Exploratory Data Analysis\n",
    "### We will first check the distribution of data of the Target Variable. i.e. Problem Category\n",
    "\n",
    "print('Target Variable Distribution - ',train['Problem Category'].value_counts())\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribution of Target Variable')\n",
    "sns.countplot(x='Problem Category',data=train)\n",
    "\n",
    "# Inference: As understood from the graph - People are asking a lot about Problem Category 1 i.e. details regarding their own house.Questions related to Problem Category 2 and 0 are not that much \n",
    "\n",
    "## Pairplot of seaborn package gives a comprehensive evaluation on what all are there and what is the trend between variables.\n",
    "## Pairplot works wonders and easier to compute when the no. of columns are less. In our condition,our columns would be ok to compute\n",
    "## and won't take much time. We have removed the following variables. -- Date related variables as I think it wont help much for now. \n",
    "## If I have some time, I was thinking of using feature selection to compute the time taken the task took to complete from the time\n",
    "## taken the task was raised. But for now, we are using the variables.\n",
    "\n",
    "variable_list=['Agent Category Assigned','Type of Request','Description of the Request','Location','Region Type','Ward No','Request Solution Category','Team Assigned','A_1','A_2']\n",
    "label=master_train['Problem Category']\n",
    "Id_test=master_test['L_Id']\n",
    "train=master_train[variable_list]\n",
    "test=master_test[variable_list]\n",
    "\n",
    "## sns.pairplot(train)\n",
    "## The above plot takes a lot to compute - However, I had run the above locally in my system. There was not much trends that were\n",
    "## showing up . One thing that was interesting is that the variables A_1 and A_2 are closely related. To prove the relation, we can \n",
    "## create a heatmap to showcase variables that are related.\n",
    "\n",
    "corr_data=train.corr()\n",
    "sns.heatmap(corr_data,annot=True)\n",
    "\n",
    "## As we can see from the heatmap, the variables A_1 and A_2 are highly correlated, therefore we need to remove it \n",
    "## By firing the below command we also find that there is only one value for Agent Category Assigned, therefore, we are \n",
    "## going to remove that as well\n",
    "\n",
    "#for variable_name in variable_list:\n",
    "    #print(variable_name,' - ',train[variable_name].value_counts())\n",
    "\n",
    "variable_list=['Type of Request','Description of the Request','Location','Region Type','Ward No','Request Solution Category','Team Assigned','A_1']\n",
    "\n",
    "train=master_train[variable_list]\n",
    "test=master_test[variable_list]\n",
    "\n",
    "## Since the count of the problem Category 1 is the highest, we will now dive into the data distribution and check if they have any \n",
    "## patterns.However, I checked some of the variables and their distribution, I was not able to identify such patterns.\n",
    "\n",
    "ax_1=sns.scatterplot(x='Ward No',y='Team Assigned',data=master_train[master_train['Problem Category']==1])\n",
    "\n",
    "ax_2=sns.scatterplot(x='Description of the Request',y='Type of Request',data=master_train[master_train['Problem Category']==1])\n",
    "\n",
    "## Please note: The below code was working fine in my local machine but for some reason,it was not working in here \n",
    "## so , I have commented it. The functionality of below code is to find the null values and then replace it using the fill na \n",
    "## function\n",
    "\n",
    "#list of columns with null values -train\n",
    "#list_of_clm_null_value=train.columns[train.isna().any()].tolist()\n",
    "#print(list_of_clm_null_value)\n",
    "\n",
    "null_columns=train.columns[train.isnull().any()]\n",
    "count=train[null_columns].isnull().sum()\n",
    "#print(count)\n",
    "\n",
    "#for i in list_of_clm_null_value:\n",
    "    #train[i].fillna(-1,inplace=True)\n",
    "\n",
    "#list of columns with null values -test\n",
    "#list_of_clm_null_value=test.columns[test.isna().any()].tolist()\n",
    "#print(list_of_clm_null_value)\n",
    "\n",
    "null_columns=test.columns[test.isnull().any()]\n",
    "count=test[null_columns].isnull().sum()\n",
    "#print(count)\n",
    "\n",
    "#for i in list_of_clm_null_value:\n",
    "    #test[i].fillna(-1,inplace=True)\n",
    "\n",
    "## This dataset is clear cut case of data imbalance. Therefore, since the weightage becomes high of the majority.\n",
    "## We are using SMOTE to even the playing field and build a better model\n",
    "## This was one technique. \n",
    "\n",
    "## We tested other models as well out of which decision tree was giving a comparative good score as compared to \n",
    "## NaiveBayes,RandomForest`,Logistic Regression etc. \n",
    "## We had ran the models in our local systems\n",
    "## SMOTE stand for Synthetic Minority Oversampling Technique\n",
    "## It is a very popular oversampling method that was proposed to improve random oversampling .\n",
    "## If we choose to undersample, there may be a chance of losing critical data that helps in defining the relationship between the \n",
    "## target variable and predictor variables for which we are choosing over-sampling. \n",
    "\n",
    "smote=SMOTE(sampling_strategy='all',random_state=47)\n",
    "os_values,os_labels=smote.fit_sample(train,label)\n",
    "\n",
    "features_train=pd.DataFrame(os_values)\n",
    "labels_train=pd.DataFrame(os_labels)\n",
    "\n",
    "DT=DecisionTreeClassifier()\n",
    "y_pred_DT=DT.fit(features_train,labels_train.values.ravel()).predict(test)\n",
    "\n",
    "\n",
    "\n",
    "clf=DecisionTreeClassifier()\n",
    "param_grid={\n",
    "        'criterion':['entropy','gini'],\n",
    "        'max_depth':[4,7,10,15,20,25,40],\n",
    "        'min_samples_split':[2,6,8,10],\n",
    "        'random_state':[47]\n",
    "}\n",
    "\n",
    "clf=GridSearchCV(clf,param_grid)\n",
    "\n",
    "y_pred_DT_tune=clf.fit(features_train,labels_train.values.ravel()).predict(test)\n",
    "\n",
    "##I am also trying XGBoost to check if the model becomes good or not. \n",
    "\n",
    "xgb=XGBClassifier()\n",
    "y_pred_xgb=xgb.fit(train,label).predict(test)\n",
    "\n",
    "##XGBoost was showing similar results but since we had tuned the DT, we will use that as our final submission\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['L_Id']=Id_test\n",
    "df['Problem Category']=y_pred_DT_tune\n",
    "\n",
    "\n",
    "#If 'df' is the dataframe containing your predictions and no feature column is for index:\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2498\n",
       "0     125\n",
       "2      53\n",
       "Name: Problem Category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
